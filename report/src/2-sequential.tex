\chapter{Sequential}
\label{cap:sequential}
In this chapter we present the main aspect of the sequential implementation to count the $n$-grams in an input.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pipeline}
Let's look the pipeline of our sequential algorithm. This was implemented in \href{https://github.com/edoardosarri24/parallel-trigrams/blob/c57c7fc52c9bcaa8fb9ba1403b795b0793713a49/sequential/src/main.c}{main.c} file.
\begin{itemize}
    \item We take a text file and we pre-precessing it. The implementation write all the characters except fot the space and the punctuation; between two token we write ony a single space. After this step we have a normalized file in \textit{data/normalized\_file.txt}.
    \item TODO
    \item We iterate throught the input and consider the current $n$-gram.
    \item We calculate the hash value of this $n$-gram.
    \item We scrool throught the whole list related to the index calculate in the previous step:
    \begin{itemize}
        \item If current $n$-gram match one already found, we increase the related counter.
        \item Althought we insert as new node in the chain and initialize the counter to one.
    \end{itemize}
    \item We pass to the next $n$-gram.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hash table}
In Section~\ref{sec:hash_table} we have defined our approch to the hash function. Now we must implent the hash table with its functionalities.

\myskip

Let's start from the key value of the function, $p$ and $M$. In general there is a rule of thumb that says to pick both values as prime numbers with M as large as possible. Starting from this:
\begin{itemize}
    \item $p$: Must be grether than the dimension of the alphabet. Thanks to normalizzation, described in Section~\ref{sec:normalizzation}, our dictionary dimention isn't to much big. We took $p=101$.
    \item $M$: The choice of $M$ determ the load factor of the hash table, that can be defined as $\alpha=\tfrac{number\_of\_stored\_element}{table\_dimension}$. We can compute in the way that of can obtain $\alpha=\tfrac{expected\_unic\_n-gram}{1.5\times expected\_unic\_n-gram}\approx0.67<1$, that usually is a good load factor. To determine the $expected\_unic\_n-gram$ the best option is use an empiracal estimate sampling from the corpus like $unic\_sampling\_n-gram\times\tfrac{total\_words}{salpled\_word}$. For our goal this isn't a foundamntal factor, so we took simply the prime number after 150K, i.e. $M=150K$.
\end{itemize}

\subsection{Overflow}
Thanks to these choice and the calculation tric for the hash function, defined in Section~\ref{sec:normalizzation}, we are sure that we haven't overflow error if we store the intermediate result in 32 bits variable.

In fact, suppose that $H_i$ can store in 32 bits we can prove that the intermediate variable used can be store in 32 bits: in the worst case we have that $intermediate\_var=(M-1)\cdot p+c_{max}<2^{32}$.

\subsection{Implementation}
The implementation of the hash table is in \href{https://github.com/edoardosarri24/parallel-trigrams/blob/c57c7fc52c9bcaa8fb9ba1403b795b0793713a49/sequential/src/hash_table.c}{hash\_table.c} file.

In addition to the code, observe that the $n$-gram dimension (i.e., the $n$ value) is define in the \textit{CMakeLists.txt} file. This give us the possibility to pass this iperparaeter from terminal we we execute the prorgam.
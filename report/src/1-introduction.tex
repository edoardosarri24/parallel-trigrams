\chapter{Introduction}
The main goal of this project is to count the $n$-gram (the occurencies of $n$ consecutive word) present in an input file. This will be addressed with two approaches: with the sequential implementation in Chapter~\ref{cap:sequential} and with the parallel implementation in Chapter~\ref{cap:parallel}. After these tow implementations we will evaluate the performance in Chapter~\ref{cap:analysis}, with particular focus on the speed-up.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pre-processing}
The pipeline that we followed for the input pre-processing consists in collect the input file and normalizating it.

\subsection{Input collection}
We have to collect some text to analyze. In our experiment we use \textit{.txt} file.

\subsection{Normalization}
\label{sec:normalizzation}
We want normalizing the input so that similar words will be rappresented by the same token, i.e., we want create equivalence classes that contain similar words. This help us to reduce the dimension of the word dictionary that composes the tringrams.

We addressed this in these ways:
\begin{itemize}
    \item Bring all characters in lower case form.
    \item Remove all the punctuation like periods, commas and exclamation points.
    \item We mantain only one type of separatore between the words, i.e. single space. This help us when we have to bound a word.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hash table}
\label{sec:hash_table}
We have to be able to count the trigrams; to address efficiently this goal we will use the hash-table.

\subsection{Hash function}
\label{sec:hash_function}
In a hash table we must define the hash function to calculate the table index of the elements. We have to calculate the index of strings, so we adopt the \textbf{Polynomial Rolling Hash}~\cite{polinomial_strings_hashing}: given the string $c_1\cdots c_n$, its hash value is defined by $$\displaystyle H(c_1\cdots c_n)=\sum_{i=1}^n(c_i\cdot p^{i-1})mod M.$$

This approch to obtain the hash value is inefficient and dangerous for the overflow. We can exploit the distributive property of the module operation: given $H(c_1\cdots c_i)$, we have that $H(c_1\cdots c_ic_{i+1})=(H(c_1\cdots c_i)\cdot p+c_{i+1})mod M$. This garantee, with the correct choice of $p$ and $M$ like the one below, the absence of overflow.

\subsection{Hash table characteristics}
Considering what was said above, the hash table has these characteristics:
\begin{itemize}
    \item The dimension of the index vector corresponde to the $M$ value.
    \item We resolve the collisions with the open chain method: every position in the table is a pointer to the data that share the same hash value.
    \item Each node of the open chain is composed by the string (i.e., the $n$-gram) as key and the counter as value.
\end{itemize}
\chapter{Introduction}
The main goal of this project is to count the $n$-gram (the occurencies of $n$ consecutive word) present in an input file. This will be addressed with two approaches: with the sequential implementation in Chapter~\ref{cap:sequential} and with the parallel implementation in Chapter~\ref{cap:parallel}. After these tow implementations we will evaluate the performance in Chapter~\ref{cap:analysis}, with particular focus on the speed-up.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Features}
Obviously the main goal is two count the $n$-gram. Others features are:
\begin{itemize}
    \item We can define the $n$ value (of the $n$-gram) at compilation time passing the parameter $N\_GRAM\_SIZE$. By default is $N\_GRAM\_SIZE=3$.
    \item At the end of computation, the system print some statistics (see Section~\ref{sec:statistics}) abount the text and abount the hash table performance.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Input and normalization}
\label{sec:normalizzation}
The code working, for semplifing the implementation, only with english sentences. The input file for our analysis was taken from \href{https://wortschatz.uni-leipzig.de/en/download/eng}{Wortschatz Leipzig project}, that contains a big corpus of english text.

\myskip

We want normalizing the input so that similar words will be rappresented by the same token, i.e., we want create equivalence classes that contain similar words. This help us to reduce the dimension of the word dictionary that composes the tringrams.

We addressed this in these ways:
\begin{itemize}
    \item Bring all characters in lower case form.
    \item Remove all the punctuation like periods, commas and exclamation points.
    \item We maintain only the numbers and the letterns.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hash table}
\label{sec:hash_table}
We have to be able to count the trigrams; to address efficiently this goal we will use the hash-table structure. We have to define the hash function to obtain the index in the has table and the caratteristichs of the hash table.

\subsection{Hash function}
\label{sec:hash_function}
In a hash table we must define the hash function to calculate the table index of the elements. We have to calculate the index of strings, so we adopt the \textbf{Polynomial Rolling Hash} technique~\cite{polinomial_strings_hashing}: given the string $c=c_1\cdots c_n$, its hash value is defined by $$\displaystyle H(c)=\sum_{i=1}^n(c_i\cdot p^{i-1})mod M.$$

This approch to obtain the hash value is inefficient and dangerous for the overflow. We can exploit the distributive property of the module operation: given $H(c_1\cdots c_i)$, we can obtain $H(c_1\cdots c_ic_{i+1})=(H(c_1\cdots c_i)\cdot p+c_{i+1})mod M$. This garantee, with the correct choice of $p$ and $M$ like the one below, the absence of overflow.

\subsection{Hash table characteristics}
Considering what was said above, the hash table has these characteristics:
\begin{itemize}
    \item The dimension of the index vector corresponde to the $M$ value.
    \item We resolve the collisions with the open chain method: every position in the table is a pointer to the data that share the same hash value.
    \item Each node of the open chain is composed by the string (i.e., the $n$-gram) as key and the counter as value.
\end{itemize}

\subsection{Choice of hyperparameters}
For the $M$ and $p$ value in generale there is a rule of thumb that says to pick both values as prime numbers with M as large as possible. Starting from this:
\begin{itemize}
    \item $p$: Must be grether than the dimension of the alphabet to reduce the probability of collisions. In the computing we analyse the corpus byte-by-byte, so if the $p$ value is larger thant 256 it will be fine. We took $p=257$, the first prime number after 256.
    \item $M$: The choice of $M$ determ the fill factor of the hash table, that can be defined as $\alpha=\tfrac{busy\_buckets}{table\_dimension}$. We compute it empirically as $M=10000019$; this choice allow us to have a good fill factor (i.e., 0.75 circa) in a big corpus in order to have a good compromise between memory usage and speed.
\end{itemize}

\subsection{Overflow}
\label{sec:overflow}
Thanks to these choice and the calculation tric for the hash function, defined in Section~\ref{sec:normalizzation}, we are sure that we haven't overflow error if we store the intermediate result in 32 bits variable.

In fact, suppose that $H_i$ can store in 32 bits we can prove that the intermediate variable used can be store in 32 bits: in the worst case we have that $intermediate\_var\_dim=(M-1)\cdot p+c_{max}<2^{32}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statistics}
\label{sec:statistics}
Once we have populated the hash table structure we have to print the statistics of our $n$-gram. In some case print the distribution (i.e., the PDF) of our dataset is usefull, but in this case this means print the frequency of all encountered word: for a big corpus this is not reccomended due to the high number of unique $n$-gram.

We have considered the below statistics:
\begin{itemize}
    \item Text statistics \\
        We cosidered the follow text statistics:
        \begin{itemize}
            \item The $K$ $n$-gram with the highest frequency (with their frequency). The parameter $K$ can be configured at compilation time with the parameter $TOP\_K$ (default $TOP\_K=10$).
            \item The number of unique $n$-gram.
        \end{itemize}
    \item Hash table performance \\
        We considered the follow statistic to evaluate the hash table performance:
        \begin{itemize}
            \item The mean and the max length of the list.
            \item Load factor, i.e. the ratio between the total elements and the total buckets. This corresponde with the average langht of chain.
            \item Fill factor, i.e. the ratio between the busy bucket and the total bucket.
        \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tools}
To validate and test our software we use two main tools: the sanitizers and the profiler.

\subsection{Sanitizers}
A sanitizer is a dynamic code analyzer integreted in the compiler that allow us to discover runtime errors in out code. The tool instruments the code to store metadata for each variable and this allow to detect runtime errors that the compiler can't see because it works in a static way.

The sanitezer is usefull during the developmet phase. In the release version we compile with the sanitizer because it introduce complexity and brings to a performance degradation.

\myskip

The sanitizers tool that we used are the following:
\begin{itemize}
    \item \textbf{Address (ASan)}: Is usefull for the detection of memory corruction errors like buffer over flow, use after free and memory leak.
    \item \textbf{Undefined (UBSan)}: Is usefull for the detection of undefined behavior in the code, i.e., partion of code that isn't conformant with the standard.
\end{itemize}

\subsection{Profiler}
Profiling is a technique for analysing the performance of our software: it is usefull to understand where the code spends most of its time and how many functions are called.

There are two techniques that profilers use: sampling, that is more efficient but less precise; instrumentation, that is very accurate but more expensive.

\myskip

In our project we used the \href{https://developer.ridgerun.com/wiki/index.php/Profiling_with_GPerfTools}{GPerfTools}. We install it with HomeBrew, but this doesn't install the \textit{pprof} tool for the analysis. The new version was found \href{https://github.com/google/pprof}{here}, and it is developed by Google in Go.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Execution}
To execute the project you have to execute one of the shell script in \href{https://github.com/edoardosarri24/parallel-trigrams/blob/master/exec}{exec folder}. The \textit{download\_data.sh} is an helper script that is used in the other.